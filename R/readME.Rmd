---
title: "readME"
output:
  md_document:
    variant: markdown_github
---

```{r}
library(doParallel)

source("C:/Users/dasil/Dropbox/code/mlVAR/mlVAR_helpers.R")


```


Using a longitudinal dataset from Intensive Longitudinal Methods: An Introduction to Diary and Experience Sampling Research (Bolger & Laurenceau), we'll randomly create some missingness to illustrate some of the functions in this repo.

```{r}

simData <- bmlm::BLch9

vars <- colnames(simData[, 3:5])

simDATA_split <- split.data.frame(simData, simData$id)

nTime <- unique(sapply(simDATA_split, nrow))

nPerson <- length(simDATA_split)

iList <- list()

for (i in 1:nPerson){
  
  jList <- list()
  
  for (j in 1:length(vars)) {
    
    perMiss <- sample(seq(.05,.5,.05),1)
    
    nMiss<- perMiss * nTime
    
    missInds <- sort(sample(1:nTime, nMiss, replace = FALSE ))
    
    tempVec <- simDATA_split[[i]][, vars[j] ] 
    
    tempVec[missInds]<- NA
    
    jList[[j]] <-  tempVec
    
  }
  
  tempDat <- do.call("cbind", jList)
  
  colnames(tempDat) <- vars
  
  tempDat <- data.frame(tempDat, ID = simDATA_split[[i]]$id, day = 1:nTime )
  
  iList[[i]]<- tempDat
  
}

simData_cmbnd <- do.call("rbind", iList)
```

Over 50% of the rows in the dataset contain at least 1 missing value and about a quarter of the data is missing in total.

```{r}
rowMiss <- apply(simData_cmbnd, 1, anyNA)

propMiss <- round(sum(rowMiss == TRUE)/ length(rowMiss), 2)

propMiss

apply(simData_cmbnd[, 1:3], 2, function(x) sum(is.na(x))/ length(x))

```

Impute the missing data using Amelia. This is about the most simple imputation one can do with Amelia. Along with speed, amelia's strength is it's ability to incorporate temporal information into the imputation model along with a host of other prior information. Further, Amelia has been shown to possess both solid imputation accuracy and model prediction accuracy relative to other imputation methods (kim et al, 2019).

```{r}
a_out <- Amelia::amelia(simData_cmbnd, cs = "ID", ts = "day" , intercs = TRUE , m = 10, p2s = FALSE)

imps <- a_out$imputations
```

We'll now use the function "par_mlVAR00" this calls the main function "mlVAR00" to fit network models and store the resulting relevant information need to create estimates from the imputed datasets (Rubin, 1987).

```{r}
doParallel::registerDoParallel(7)

bootImp <- foreach(i=1:length(imps)) %dopar% par_mlVAR00(dat = imps[[i]], scale = TRUE, variables = vars, ID = "ID", rfStructure = c("correlated", "correlated"), timeArgs = list(NULL, NULL, FALSE))
```


Combine the estimates according to rubins rule (rubin, 1987).

```{r}

combined_ests <- rubin_combine(bootImp, m =length(imps))

list(temporal = combined_ests$temporal$t_value, contemporaneous = combined_ests$contemporaneous_network$t_value, betweenSubjects = combined_ests$`between-subjects_network`$t_value)

```

Compare the test-statistics from the imputed data to the complete dataset

```{r}
completeData <- mlVAR00(dat = simData, scale = TRUE, variables = vars, ID = "id", temporal = "correlated", contemporaneous = "correlated")

list(temporal = completeData$results$temporal$`T-value`, contemporaneous = completeData$results$contemporaneous$`T-value`, betweenSubjects = completeData$results$`between-subjects`$`T-value`)

```

Finally, temporal information can be included in the model through the "time" arguments. We can now again fit another model - this time including time and time^2 as additional fixed effects along with their respective random slopes.

```{r}
completeData_time <- mlVAR00(simData, variables = vars, ID = "id", temporal = "correlated", contemporaneous = "correlated" , scale = TRUE, timeVar = "time", timePoly = 2, timeRandom = TRUE )

list(temporal = completeData_time$results$temporal$`T-value`, contemporaneous = completeData_time$results$contemporaneous$`T-value`, betweenSubjects = completeData_time$results$`between-subjects`$`T-value`)


```


